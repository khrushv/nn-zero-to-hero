{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bb7b54e-0f05-48cb-a437-5b1e48a8d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8c93767-60bd-4122-a88c-5bb063e2aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "def get_dataset(names):\n",
    "    xs, ys = [], []\n",
    "    for name in names:\n",
    "        name = '..' + name + '.'\n",
    "        for chr1, chr2, chr3 in zip(name, name[1:], name[2:]):\n",
    "            idx1 = chtoi[chr1]\n",
    "            idx2 = chtoi[chr2]\n",
    "            idx3 = chtoi[chr3]\n",
    "            xs.append((idx1, idx2))\n",
    "            ys.append(idx3)\n",
    "    return xs, ys\n",
    "\n",
    "def predict_name(g, get_sample_distribution):\n",
    "    name = '..'\n",
    "    idx1, idx2 = 0, 0\n",
    "    while True:\n",
    "        sample_distribution = get_sample_distribution(idx1, idx2)\n",
    "        idx = torch.multinomial(sample_distribution, num_samples=1, replacement=True, generator=g).item()\n",
    "        idx1 = idx2\n",
    "        idx2 = idx\n",
    "        name += itoch[idx]\n",
    "        if idx == 0:\n",
    "            break\n",
    "    return name\n",
    "\n",
    "def get_loss(probs, y_s):\n",
    "    return -probs[torch.arange(probs.shape[0]), y_s].log().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74a5934-37fb-4c95-9263-24691563a472",
   "metadata": {},
   "source": [
    "## E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f5c03c-d33b-4d6f-bc6a-1b2515645e96",
   "metadata": {},
   "source": [
    "## Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb145598-1061-4f9a-8935-80794a1cc412",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c1260e-f5d6-438f-a2ad-d8c685c369a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = set(''.join(names))\n",
    "unique_chars = sorted(unique_chars)\n",
    "chtoi = {ch: i + 1 for i, ch in enumerate(unique_chars)}\n",
    "chtoi['.'] = 0\n",
    "itoch = {value:key for key, value in chtoi.items()}\n",
    "table_counts = torch.zeros(len(chtoi), len(chtoi), len(chtoi), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "001d7523-0446-43f1-8ac7-fef5c4b4dfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = get_dataset(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e2e6c7b-bf02-48b7-b724-85654eff3e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip(xs, ys):\n",
    "    idx1 = x[0]\n",
    "    idx2 = x[1]\n",
    "    table_counts[idx1][idx2][y] += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24e7f768-4430-4b99-a7c1-998baab5241a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..mip.\n",
      "..axx.\n",
      "..mereyannyaar.\n",
      "..knooraen.\n",
      "..el.\n",
      "..marviovania.\n",
      "..odarimalaalexiaganilley.\n",
      "..helahroni.\n",
      "..haat.\n",
      "..affiya.\n",
      "..isemarrisleemikh.\n",
      "..bech.\n",
      "..amilleia.\n",
      "..trutandenneppalycethon.\n",
      "..jan.\n",
      "..kryn.\n",
      "..yusehanii.\n",
      "..laymira.\n",
      "..knoenoah.\n",
      "..nowynni.\n"
     ]
    }
   ],
   "source": [
    "probas = table_counts / table_counts.sum(dim=2, keepdim=True)\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for _ in range(20):\n",
    "    name = predict_name(g, lambda idx1, idx2: probas[idx1][idx2])\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bd83340-b9f0-465a-aa9b-45248f037f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1857433319091797\n"
     ]
    }
   ],
   "source": [
    "#trigram loss\n",
    "probs = []\n",
    "for x, y in zip(xs, ys):\n",
    "    proba = probas[x].unsqueeze(dim=0)\n",
    "    probs.append(proba)\n",
    "probs = torch.cat(probs)\n",
    "loss = get_loss(probs, ys).item()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e31859a-e0a7-4bca-8ded-79c8629f7889",
   "metadata": {},
   "source": [
    "## Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1e7f6e6-a9de-4285-b41f-25a7ec010d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def map_idxs(idx1, idx2, vocab_len):\n",
    "#    return len(chtoi) * idx1 + idx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3e55b91-5e84-4215-9ab2-ff7a646569f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s, y_s = get_dataset(names)\n",
    "#x_s = [map_idxs(idx1, idx2, len(chtoi) for idx1, idx2 in x_s]\n",
    "x_s = torch.tensor(x_s)\n",
    "y_s = torch.tensor(y_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a729308-6d84-41d6-ab2f-5d2f49c31db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb_size = len(chtoi)**2\n",
    "#W = torch.rand(emb_size, len(chtoi), generator=g, requires_grad=True, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2131263-73ce-4a99-9303-2771624fbe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The basic idea is to represent each character as a one-hot vector of length 27, and then concatenate them together\n",
    "# in this way the final length of input for matrix w will be 27 * 2\n",
    "W = torch.rand(27*2, len(chtoi), generator=g, requires_grad=True, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ff254b7-93d6-4dcb-83d9-2c1e7e6f688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(xenc):\n",
    "    counts = xenc @ W\n",
    "    logits = counts.exp()\n",
    "    probs = logits / torch.sum(logits, dim=1, keepdim=True)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d276e5ab-e1e7-4ddf-bb7c-3884a48e8c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc1 = F.one_hot(x_s[:, 0], num_classes=len(chtoi)).float()\n",
    "xenc2 = F.one_hot(x_s[:, 1], num_classes=len(chtoi)).float()\n",
    "xenc = torch.cat((xenc1, xenc2), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fe68128-37e7-4c6f-85b7-4a721a731ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration num - 0: Training loss: 3.4063494205474854\n",
      "Iteration num - 100: Training loss: 2.383622884750366\n",
      "Iteration num - 200: Training loss: 2.361172676086426\n",
      "Iteration num - 300: Training loss: 2.35288667678833\n",
      "Iteration num - 400: Training loss: 2.348541021347046\n",
      "Iteration num - 500: Training loss: 2.3458638191223145\n",
      "Iteration num - 600: Training loss: 2.344055652618408\n",
      "Iteration num - 700: Training loss: 2.3427603244781494\n",
      "Iteration num - 800: Training loss: 2.341792345046997\n",
      "Iteration num - 900: Training loss: 2.3410470485687256\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    probs = forward(xenc)\n",
    "    loss = get_loss(probs, y_s)\n",
    "    W.grad = None\n",
    "    #b.grad = None\n",
    "    loss.backward()\n",
    "    W.data -=  20 * W.grad\n",
    "    if i % 100 == 0:\n",
    "        print(f'Iteration num - {i}: Training loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19f43724-8283-45a6-b40d-c9aa86e559d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we didn't achieve the same loss as trigram model, i think this is because of the insufficient complexity of our model. Initially, i tried \n",
    "# an architecture with 27**2 input, i.e. for each combination of 27 chars - vector. This approach yielded a loss close to that of the trigram model. \n",
    "# However, I find the current architecture more intriguingðŸ˜…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f84dcb7-ea79-40d6-b068-79fe1c5a9909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_get_sample_distribution(idx1, idx2):\n",
    "    xenc1 = F.one_hot(torch.tensor(idx1).unsqueeze(dim=0), num_classes=len(chtoi)).float()\n",
    "    xenc2 = F.one_hot(torch.tensor(idx2).unsqueeze(dim=0), num_classes=len(chtoi)).float()\n",
    "    xenc = torch.cat((xenc1, xenc2), dim=1)\n",
    "    probs = forward(xenc)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a9c8c8d-9af5-42cd-9e47-6309f28f8842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..mor.\n",
      "..ays.\n",
      "..minaymnnyaes.\n",
      "..konamaloe.\n",
      "..caonavioy.\n",
      "..rie.\n",
      "..oi.\n",
      "..ondalaek.\n",
      "..shalekirierielah.\n",
      "..yshi.\n",
      "..haat.\n",
      "..ah.\n",
      "..kyn.\n",
      "..ilan.\n",
      "..lumiogengen.\n",
      "..be.\n",
      "..fa.\n",
      "..jilgila.\n",
      "..tostandemkrmya.\n",
      "..zacrevina.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for _ in range(20):\n",
    "    name = predict_name(g, nn_get_sample_distribution)\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed8143a-14e1-4e3d-94db-8d184566a940",
   "metadata": {},
   "source": [
    "## E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c720ecd-0113-49a0-adaf-0257bfc7d2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# code from lecture\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "num_train_elements = int(0.8 * xs.shape[0])\n",
    "num_val_elements = int(0.9 * xs.shape[0])\n",
    "train_xs = xs[:num_train_elements]\n",
    "train_y = ys[:num_train_elements]\n",
    "val_xs = xs[num_train_elements:num_val_elements]\n",
    "val_y = ys[num_train_elements:num_val_elements]\n",
    "test_xs = xs[num_val_elements:]\n",
    "test_y = ys[num_val_elements:]\n",
    "\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25d18b22-3f70-4cc2-8e50-c2d8ee3cd372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration num - 999: Training loss: 3.777691125869751\n",
      "Iteration num - 999: Training loss: 2.666555643081665\n",
      "Iteration num - 999: Training loss: 2.553800582885742\n",
      "Iteration num - 999: Training loss: 2.51287841796875\n",
      "Iteration num - 999: Training loss: 2.4926834106445312\n",
      "Iteration num - 999: Training loss: 2.481161594390869\n",
      "Iteration num - 999: Training loss: 2.473928213119507\n",
      "Iteration num - 999: Training loss: 2.469041585922241\n",
      "Iteration num - 999: Training loss: 2.4655580520629883\n",
      "Iteration num - 999: Training loss: 2.4629757404327393\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "xenc = F.one_hot(train_xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "for k in range(100):\n",
    "  \n",
    "  # forward pass\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(xenc.shape[0]), train_y].log().mean() + 0.01*(W**2).mean()\n",
    "  if k % 10 == 0:\n",
    "      print(f'Iteration num - {k}: Training loss: {loss}')\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a522e48-07c6-4de5-a8d6-4a68e7b4dece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on val = 2.612541913986206\n",
      "loss on test = 2.616680145263672\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for xs, ys, split_name in [(val_xs, val_y, 'val'), (test_xs, test_y, 'test')]:\n",
    "    with torch.no_grad():\n",
    "        xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "          \n",
    "        # forward pass\n",
    "        logits = xenc @ W # predict log-counts\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "        loss = -probs[torch.arange(xenc.shape[0]), ys].log().mean()\n",
    "        print(f'loss on {split_name} = {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f3daabd-fe5f-493b-a4d1-f0664aee28f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss on val and test is almost the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bb4fc3-5290-4df9-9519-a4fb03e955d1",
   "metadata": {},
   "source": [
    "### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2da2faa2-e64f-41b7-b8f4-2f0682f459b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = get_dataset(names)\n",
    "#x_s = [map_idxs(idx1, idx2, len(chtoi) for idx1, idx2 in x_s]\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "num_train_elements = int(0.8 * xs.shape[0])\n",
    "num_val_elements = int(0.9 * xs.shape[0])\n",
    "train_xs = xs[:num_train_elements]\n",
    "train_y = ys[:num_train_elements]\n",
    "val_xs = xs[num_train_elements:num_val_elements]\n",
    "val_y = ys[num_train_elements:num_val_elements]\n",
    "test_xs = xs[num_val_elements:]\n",
    "test_y = ys[num_val_elements:]\n",
    "\n",
    "xenc1 = F.one_hot(train_xs[:, 0], num_classes=len(chtoi)).float()\n",
    "xenc2 = F.one_hot(train_xs[:, 1], num_classes=len(chtoi)).float()\n",
    "xenc_train = torch.cat((xenc1, xenc2), dim=1)\n",
    "\n",
    "xenc1 = F.one_hot(val_xs[:, 0], num_classes=len(chtoi)).float()\n",
    "xenc2 = F.one_hot(val_xs[:, 1], num_classes=len(chtoi)).float()\n",
    "xenc_val = torch.cat((xenc1, xenc2), dim=1)\n",
    "\n",
    "xenc1 = F.one_hot(test_xs[:, 0], num_classes=len(chtoi)).float()\n",
    "xenc2 = F.one_hot(test_xs[:, 1], num_classes=len(chtoi)).float()\n",
    "xenc_test = torch.cat((xenc1, xenc2), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61ecaa41-7519-4334-b7be-0d6b9272bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.rand(27*2, len(chtoi), generator=g, requires_grad=True, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48be3cd8-c8dd-4c73-bc91-ad06fbf838a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration num - 0: Training loss: 3.4317004680633545\n",
      "Iteration num - 10: Training loss: 2.4659430980682373\n",
      "Iteration num - 20: Training loss: 2.422708749771118\n",
      "Iteration num - 30: Training loss: 2.367277145385742\n",
      "Iteration num - 40: Training loss: 2.3584823608398438\n",
      "Iteration num - 50: Training loss: 2.3519515991210938\n",
      "Iteration num - 60: Training loss: 2.34675669670105\n",
      "Iteration num - 70: Training loss: 2.3425135612487793\n",
      "Iteration num - 80: Training loss: 2.3389763832092285\n",
      "Iteration num - 90: Training loss: 2.3359792232513428\n",
      "Iteration num - 100: Training loss: 2.333405017852783\n",
      "Iteration num - 110: Training loss: 2.3311684131622314\n",
      "Iteration num - 120: Training loss: 2.3292059898376465\n",
      "Iteration num - 130: Training loss: 2.327470302581787\n",
      "Iteration num - 140: Training loss: 2.325923204421997\n",
      "Iteration num - 150: Training loss: 2.324535846710205\n",
      "Iteration num - 160: Training loss: 2.323284149169922\n",
      "Iteration num - 170: Training loss: 2.3221497535705566\n",
      "Iteration num - 180: Training loss: 2.3211171627044678\n",
      "Iteration num - 190: Training loss: 2.32017183303833\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    probs = forward(xenc_train)\n",
    "    loss = get_loss(probs, train_y)\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    if i <= 25:\n",
    "        lr = 50\n",
    "    else:\n",
    "        lr = 25\n",
    "    W.data -=  lr * W.grad\n",
    "    if i % 10 == 0:\n",
    "        print(f'Iteration num - {i}: Training loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b177ca81-c8ea-46eb-a2d7-686f94808c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on val = 2.5141124725341797\n",
      "loss on test = 2.5235397815704346\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for xs, ys, split_name in [(xenc_val, val_y, 'val'), (xenc_test, test_y, 'test')]:\n",
    "    with torch.no_grad():\n",
    "        probs = forward(xs)\n",
    "        loss = get_loss(probs, ys)\n",
    "        print(f'loss on {split_name} = {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb7ea3cd-7209-4165-886c-4d474216ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference a bit larger than expected, but i believe it's still close enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ec489e-ae56-4f80-a5d5-15a52e744ffe",
   "metadata": {},
   "source": [
    "## E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "161e3828-4fce-4df2-a163-0ad836c0a666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strenght - 1e-05, train_loss - 2.31939959526062, val_loss - 2.5141122341156006\n",
      "strenght - 0.001, train_loss - 2.3204896450042725, val_loss - 2.514098882675171\n",
      "strenght - 0.01, train_loss - 2.3298182487487793, val_loss - 2.514075517654419\n",
      "strenght - 0.1, train_loss - 2.3896865844726562, val_loss - 2.5200507640838623\n",
      "strenght - 1.0, train_loss - 2.6047399044036865, val_loss - 2.614614963531494\n",
      "strenght - 2.0, train_loss - 2.7164664268493652, val_loss - 2.6820991039276123\n"
     ]
    }
   ],
   "source": [
    "strenght_to_loss = {}\n",
    "for strenght in [0.00001, 0.001, 0.01, 0.1, 1.0, 2.0]:\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    W = torch.rand(27*2, len(chtoi), generator=g, requires_grad=True, dtype=torch.float)\n",
    "    for i in range(200):\n",
    "        probs = forward(xenc_train)\n",
    "        loss = get_loss(probs, train_y) + strenght * (W**2).mean()\n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "        if i <= 25:\n",
    "            lr = 50\n",
    "        else:\n",
    "            lr = 25\n",
    "        W.data -=  lr * W.grad\n",
    "    with torch.no_grad():\n",
    "        val_probs = forward(xenc_val)\n",
    "        val_loss = get_loss(val_probs, val_y)\n",
    "        strenght_to_loss[strenght] = val_loss\n",
    "    print(f'strenght - {strenght}, train_loss - {loss}, val_loss - {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "848d037a-6191-4ecc-9784-4778e3c8cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that as the regularization strength increases, the loss on the training dataset also increases\n",
    "# The reason is that it becomes harder for the model to overfit because of regularization on the weights\n",
    "# And for val dataset there exists an optimal value outside of which the loss increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42ae9441-18b9-421c-b2fb-8c33d8373fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.rand(27*2, len(chtoi), generator=g, requires_grad=True, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efa4d295-5682-48fd-a206-ed0fce79d2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.4 s, sys: 4.54 s, total: 18.9 s\n",
      "Wall time: 18.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(200):\n",
    "    probs = forward(xenc_train)\n",
    "    loss = get_loss(probs, train_y) + 0.01 * (W**2).mean()\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    if i <= 25:\n",
    "        lr = 50\n",
    "    else:\n",
    "        lr = 25\n",
    "    W.data -=  lr * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c111bf9-6748-4e9a-a7f8-5887c1affd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss - 2.523268461227417\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_probs = forward(xenc_test)\n",
    "    test_loss = get_loss(test_probs, test_y)\n",
    "print(f'test_loss - {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a367f6b6-ec44-4ace-9b14-b27b8f003be5",
   "metadata": {},
   "source": [
    "## E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffe6d7c7-e892-4079-ad9f-f3b9945ed7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.rand(2, 27, len(chtoi), generator=g, requires_grad=True, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "800eafe4-191f-4c24-8321-771fc5894416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration num - 0: Training loss: 3.435056209564209\n",
      "Iteration num - 10: Training loss: 2.4715957641601562\n",
      "Iteration num - 20: Training loss: 2.4289772510528564\n",
      "Iteration num - 30: Training loss: 2.3741769790649414\n",
      "Iteration num - 40: Training loss: 2.3657381534576416\n",
      "Iteration num - 50: Training loss: 2.3595263957977295\n",
      "Iteration num - 60: Training loss: 2.3546226024627686\n",
      "Iteration num - 70: Training loss: 2.35064697265625\n",
      "Iteration num - 80: Training loss: 2.3473575115203857\n",
      "Iteration num - 90: Training loss: 2.344590663909912\n",
      "Iteration num - 100: Training loss: 2.3422319889068604\n",
      "Iteration num - 110: Training loss: 2.340198040008545\n",
      "Iteration num - 120: Training loss: 2.3384268283843994\n",
      "Iteration num - 130: Training loss: 2.336871385574341\n",
      "Iteration num - 140: Training loss: 2.335495948791504\n",
      "Iteration num - 150: Training loss: 2.334270715713501\n",
      "Iteration num - 160: Training loss: 2.333173990249634\n",
      "Iteration num - 170: Training loss: 2.3321869373321533\n",
      "Iteration num - 180: Training loss: 2.3312950134277344\n",
      "Iteration num - 190: Training loss: 2.3304848670959473\n",
      "CPU times: user 15.5 s, sys: 8.65 s, total: 24.2 s\n",
      "Wall time: 24.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(200):\n",
    "    counts = W[0][train_xs[:, 0]] + W[1][train_xs[:, 1]]\n",
    "    logits = counts.exp()\n",
    "    probs = logits / torch.sum(logits, dim=1, keepdim=True)\n",
    "    loss = get_loss(probs, train_y) + 0.01 * (W**2).mean()\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    if i <= 25:\n",
    "        lr = 50\n",
    "    else:\n",
    "        lr = 25\n",
    "    W.data -=  lr * W.grad\n",
    "    if i % 10 == 0:\n",
    "        print(f'Iteration num - {i}: Training loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09e9e167-d55e-4b2d-830a-144856ca3d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss - 2.523268699645996\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    counts = W[0][test_xs[:, 0]] + W[1][test_xs[:, 1]]\n",
    "    logits = counts.exp()\n",
    "    test_probs = logits / torch.sum(logits, dim=1, keepdim=True)\n",
    "    test_loss = get_loss(test_probs, test_y)\n",
    "print(f'test_loss - {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85cf587-22a9-41cf-ab37-69f6c5dce7b1",
   "metadata": {},
   "source": [
    "## E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "141dce6a-cdc6-4b55-9dcd-ceb334443083",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.rand(2, 27, len(chtoi), generator=g, requires_grad=True, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b20a9f7-6f50-4818-bc00-2a886282153e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration num - 0: Training loss: 3.435056686401367\n",
      "Iteration num - 10: Training loss: 2.4715957641601562\n",
      "Iteration num - 20: Training loss: 2.4289772510528564\n",
      "Iteration num - 30: Training loss: 2.3741767406463623\n",
      "Iteration num - 40: Training loss: 2.3657381534576416\n",
      "Iteration num - 50: Training loss: 2.3595263957977295\n",
      "Iteration num - 60: Training loss: 2.3546223640441895\n",
      "Iteration num - 70: Training loss: 2.35064697265625\n",
      "Iteration num - 80: Training loss: 2.3473572731018066\n",
      "Iteration num - 90: Training loss: 2.344590663909912\n",
      "Iteration num - 100: Training loss: 2.3422319889068604\n",
      "Iteration num - 110: Training loss: 2.340198040008545\n",
      "Iteration num - 120: Training loss: 2.3384268283843994\n",
      "Iteration num - 130: Training loss: 2.3368711471557617\n",
      "Iteration num - 140: Training loss: 2.335495710372925\n",
      "Iteration num - 150: Training loss: 2.334270477294922\n",
      "Iteration num - 160: Training loss: 2.3331737518310547\n",
      "Iteration num - 170: Training loss: 2.3321871757507324\n",
      "Iteration num - 180: Training loss: 2.3312947750091553\n",
      "Iteration num - 190: Training loss: 2.3304848670959473\n",
      "CPU times: user 12 s, sys: 16 ms, total: 12 s\n",
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(200):\n",
    "    counts = W[0][train_xs[:, 0]] + W[1][train_xs[:, 1]]\n",
    "    loss = F.cross_entropy(input=counts, target=train_y) + 0.01 * (W**2).mean()\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    if i <= 25:\n",
    "        lr = 50\n",
    "    else:\n",
    "        lr = 25\n",
    "    W.data -=  lr * W.grad\n",
    "    if i % 10 == 0:\n",
    "        print(f'Iteration num - {i}: Training loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b550cdd-0995-4f82-aa9e-fdda718f5887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss - 2.523268699645996\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    counts = W[0][test_xs[:, 0]] + W[1][test_xs[:, 1]]\n",
    "    test_loss = F.cross_entropy(input=counts, target=test_y)\n",
    "print(f'test_loss - {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb2bc483-7484-4870-9333-e02d3b1f3dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'd prefer to use  F.cross_entropy because of effectiveness forward and backward passes, as well as its numerical stability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
